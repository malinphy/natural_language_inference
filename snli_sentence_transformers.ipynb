{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y5Giwb4z4PbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf \n",
        "from tensorflow import keras \n",
        "from tensorflow.keras import Model, layers, Input\n",
        "from tensorflow.keras.layers import *\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "use  = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\") ## universal sentence encoder model\n",
        "\n",
        "# from model import model\n",
        "import pickle"
      ],
      "metadata": {
        "id": "kCj02Hp-DdRj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2970dd83-477a-49b3-b148-b21c7d50bb6a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('tf version:',tf.__version__)\n",
        "print('tf_hub version:',hub.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkqsQmGH3t8W",
        "outputId": "83b80dcf-59c4-401d-cf7c-fa422846c765"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf version: 2.8.2\n",
            "tf_hub version: 0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = 'drive/MyDrive/Colab Notebooks/datasets/snli/snli_1.0_train.csv'\n",
        "test_path = 'drive/MyDrive/Colab Notebooks/datasets/snli/snli_1.0_test.csv'\n",
        "validation_path = 'drive/MyDrive/Colab Notebooks/datasets/snli/snli_1.0_dev.csv'\n",
        "\n",
        "selected_columns = ['sentence1','sentence2','label1']\n",
        "\n",
        "train_df = pd.read_csv(train_path,usecols = selected_columns)\n",
        "train_df = train_df.rename(columns={\"sentence1\": \"premise\", \"sentence2\": \"hypothesis\", \"label1\": \"label\"})\n",
        "test_df =  pd.read_csv(test_path,usecols = selected_columns)\n",
        "test_df = test_df.rename(columns={\"sentence1\": \"premise\", \"sentence2\": \"hypothesis\", \"label1\": \"label\"})\n",
        "validation_df =  pd.read_csv(validation_path,usecols = selected_columns)\n",
        "validation_df = validation_df.rename(columns={\"sentence1\": \"premise\", \"sentence2\": \"hypothesis\", \"label1\": \"label\"})\n"
      ],
      "metadata": {
        "id": "U14Fj7EUDrST"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LE = LabelEncoder()\n",
        "LE.fit(train_df['label'])\n",
        "train_df['label'] = enc_label = LE.transform(train_df['label'])\n",
        "test_df['label'] = enc_label = LE.transform(test_df['label'])\n",
        "validation_df['label'] = enc_label = LE.transform(validation_df['label'])\n",
        "\n",
        "\n",
        "output = open('LE.pkl', 'wb')\n",
        "pickle.dump(LE, output)\n",
        "output.close()\n",
        "\n",
        "pkl_file = open('LE.pkl', 'rb')\n",
        "LE = pickle.load(pkl_file) \n",
        "pkl_file.close()"
      ],
      "metadata": {
        "id": "B8G63WT5v7j2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# conv_size = 10\n",
        "conv_size = 40\n",
        "drop_rate = 0.2\n",
        "pool_size = 6"
      ],
      "metadata": {
        "id": "LMRNFSFkqlFc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_df))\n",
        "train_df = train_df.dropna().reset_index(drop = True)\n",
        "len(train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KyltuMeqmn9",
        "outputId": "fcf8535e-90ff-49a9-d650-33c75d11a5ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "550152\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "550146"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model():\n",
        "    left_input = Input(shape = (), name = 'left_input', dtype = tf.string)\n",
        "    right_input = Input(shape = (), name = 'right_input', dtype = tf.string)\n",
        "\n",
        "    encoder_layer = hub.KerasLayer(use, trainable = True)\n",
        "\n",
        "    left_encoder = encoder_layer(left_input)\n",
        "    right_encoder = encoder_layer(right_input)\n",
        "    subtract_layer = tf.keras.layers.Subtract()([left_encoder, right_encoder])\n",
        "    subtract_layer = tf.math.abs(subtract_layer)\n",
        "\n",
        "    left_encoder_wide = tf.expand_dims(left_encoder,axis=-1)\n",
        "    right_encoder_wide = tf.expand_dims(right_encoder,axis=-1)\n",
        "    subtract_wide = tf.expand_dims(subtract_layer,axis=-1)\n",
        "\n",
        "    left_convo  = Conv1D(512, conv_size)(left_encoder_wide)\n",
        "    left_pool = MaxPool1D(pool_size)(left_convo)\n",
        "\n",
        "    right_convo  = Conv1D(512, conv_size)(right_encoder_wide)\n",
        "    right_pool = MaxPool1D(pool_size)(right_convo)\n",
        "\n",
        "    sub_convo = Conv1D(512, conv_size)(subtract_wide)\n",
        "    sub_pool = MaxPool1D(pool_size)(sub_convo)\n",
        "\n",
        "    left_pool = Flatten()(left_pool)\n",
        "    right_pool = Flatten()(right_pool)\n",
        "    sub_pool = Flatten()(sub_pool)\n",
        "\n",
        "\n",
        "\n",
        "    concat_layer = tf.keras.layers.Concatenate(axis=-1)([left_pool, right_pool,sub_pool])\n",
        "\n",
        "\n",
        "    d2_layer = Dense(512, activation = 'relu')(concat_layer)\n",
        "    d2_layer = Dropout(drop_rate)(d2_layer)\n",
        "    d3_layer = Dense(256, activation = 'relu')(d2_layer)\n",
        "    d3_layer = Dropout(drop_rate)(d3_layer)\n",
        "    d3_layer = Dense(64, activation = 'relu')(d2_layer)\n",
        "    d3_layer = Dropout(drop_rate)(d3_layer)\n",
        "    d4_layer = Dense(3, activation = 'softmax')(d3_layer)\n",
        "\n",
        "    return Model(inputs = [left_input, right_input], outputs = d4_layer)"
      ],
      "metadata": {
        "id": "-kPu2170FJ__"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_model = model()\n",
        "use_model.compile(\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "                                        # learning_rate = 0.006\n",
        "    ),\n",
        "    metrics= ['accuracy'])\n",
        "\n",
        "history=use_model.fit(\n",
        "    [train_df['premise'], train_df['hypothesis']],\n",
        "    train_df['label'],\n",
        "    epochs = 2,\n",
        "    batch_size = 20,\n",
        "    validation_split = 0.2\n",
        "                    )\n",
        "\n",
        "\n",
        "use_model.save_weights('drive/MyDrive/Colab Notebooks/snli/USE_snli_weights.h5')"
      ],
      "metadata": {
        "id": "ZQh0GI80qneK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_model.load_weights('drive/MyDrive/Colab Notebooks/snli/USE_snli_weights.h5')"
      ],
      "metadata": {
        "id": "gzdd1EfJrjuQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = tf.math.top_k(use_model.predict([test_df['premise'], test_df['hypothesis']]),k=1)[1]"
      ],
      "metadata": {
        "id": "7oqGaI1eqnYo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(test_df['label'], predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_1n6yGgqsJl",
        "outputId": "b28f4279-4fdf-4afa-ad59-83ba057d4ee1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2537,  339,  457],\n",
              "       [  82, 2885,  366],\n",
              "       [ 231,  618, 2485]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('F1 SCORE :',f1_score(test_df['label'], predictions, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_JzkSX1qsDI",
        "outputId": "6fa531f4-e4e9-4eaf-fbc8-8962d6e117ca"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 SCORE : 0.7910290031945397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(classification_report(test_df['label'], predictions, target_names=LE.inverse_transform([0,1,2])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2WnTiAJ_F9M",
        "outputId": "f70b35c8-4c63-4077-d103-1c520f95cea1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "contradiction       0.89      0.76      0.82      3333\n",
            "   entailment       0.75      0.87      0.80      3333\n",
            "      neutral       0.75      0.75      0.75      3334\n",
            "\n",
            "     accuracy                           0.79     10000\n",
            "    macro avg       0.80      0.79      0.79     10000\n",
            " weighted avg       0.80      0.79      0.79     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oC_A7oZXF85n"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gMC4Uejz6pQ-"
      },
      "execution_count": 14,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}